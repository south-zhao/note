## Scrapy与CrawlSpider

CrawlSpider其实是Spider的一个子类，除了继承到Spider的特性和功能外，还派生除了其自己独有的更加强大的特性和功能。其中最显著的功能就是”**LinkExtractors链接提取器**“。Spider是所有爬虫的基类，其设计原则只是为了爬取start_url列表中网页，而从爬取到的网页中提取出的url进行继续的爬取工作使用CrawlSpider更合适



**项目创建流程**

```shell
    1.创建scrapy工程(cmd切换到要创建项目的文件夹下执行)：scrapy startproject projectName       （如：scrapy startproject  crawlPro）

    2.创建爬虫文件(cmd切换到创建的项目下执行)：scrapy genspider -t crawl spiderName www.xxx.com     (如：scrapy genspider -t crawl crawlDemo www.qiushibaike.com)

　　　　--此指令对比以前的指令多了 "-t crawl"，表示创建的爬虫文件是基于CrawlSpider这个类的，而不再是Spider这个基类。

    3.启动爬虫文件(cmd基于步骤二的路径执行)：scrapy crawl crawlDemo     (启动的一定是name对应的值，如果爬虫文件与name的值不一致，任然以name的值进行启动)
```



### 生成的爬虫文件参数介绍

```python
LinkExtractor：顾名思义，链接提取器。

LinkExtractor(

　　　　　　　  allow=r'Items/'，# 满足括号中“正则表达式”的值会被提取，如果为空，则全部匹配。

　　　　　　　　 deny=xxx,  # 满足正则表达式的则不会被提取。

　　　　　　　　 restrict_xpaths=xxx, # 满足xpath表达式的值会被提取

　　　　　　　　 restrict_css=xxx, # 满足css表达式的值会被提取

　　　　　　　　 deny_domains=xxx, # 不会被提取的链接的domains。　

　　  )
　　　　- 作用：提取response中符合规则的链接。　

Rule : 规则解析器。根据链接提取器中提取到的链接，根据指定规则提取解析器链接网页中的内容。

　　　　 Rule(LinkExtractor(allow=r'Items/'), callback='parse_item', follow=True)

　　　　- 参数介绍：

　　　　　　参数1：指定链接提取器

　　　　　　参数2：指定规则解析器解析数据的规则（回调函数）

　　　　　　参数3：是否将链接提取器继续作用到链接提取器提取出的链接网页中。当callback为None,参数3的默认值为true。

rules=( ):指定不同规则解析器。一个Rule对象表示一种提取规则。

CrawlSpider整体爬取流程：

　　　　a)爬虫文件首先根据起始url，获取该url的网页内容

　　　　b)链接提取器会根据指定提取规则将步骤a中网页内容中的链接进行提取

　　　　c)规则解析器会根据指定解析规则将链接提取器中提取到的链接中的网页内容根据指定的规则进行解析

　　　　d)将解析数据封装到item中，然后提交给管道进行持久化存储
```



## 回顾与重点声明

```python
1.scrapy的crawlspider爬虫
    1.crawlspider是什么？
        回顾之前的代码，很大一部分时间在找下一页的url地址或者是内容上面的url地址
    2.crawlspider思路
        1.从response中提取所有满足规则的url地址
        2.自动的构造自己的requests请求，发送给引擎，同时能够指定callback函数
        即：crawlspider爬虫可以按照规则自动获取数据
    3.创建命令：
        scrapy genspider -t crawl job 163.com
    4.观察与scrapy爬虫的区别
        1.没有parse方法，多了一个rule对象
        2.rules对象可以直接理解为规则对像
    https://hr.163.com/position/list.do
    
    作用：
        链接提取器：提取url地址
    
    5.爬虫文件说明
        rules：
            rules = (
        Rule(LinkExtractor(allow=r'\?currentPage=\d+$'), callback='parse_item', follow=True),
        # 1. rules是一个元组或者是列表，包含的是Rule对象
        # 2. Rule表示规则，其中包含LinkExtractor,callback和follow等参数
        # 3. LinkExtractor:连接提取器，可以通过正则或者是xpath来进行url地址的匹配
        # 4. callback :表示经过连接提取器提取出来的url地址响应的回调函数，可以没有，没有表示响应不会进行回调函数的处理
        # 5. follow：连接提取器提取的url地址对应的响应是否还会继续被rules中的规则进行提取，True表示会，Flase表示不会
    )

    6.注意点：
        1.爬虫文件的创建命令
            普通 scrapy genspider + 爬虫名 + 爬虫域
            scrapy genspider crawl -t + 爬虫名 + 爬虫域
        2.crawlspider不能再有parse方法, 该方法被crawlspider用来实现基础url提取功能
            如果定义了parse方法，就是改写父类方法，爬虫会报错
        3.rule对象中，链接提取器LinkExtractor为固定参数，其他的callback，follow参数可有可无
        4.不指定callback且follow为True的情况下，满足rules中规则的url还会被继续提取和请求
        5.如果一个被提取的url满足多个Rule，那么会从rules中选择一个满足匹配条件的Rule执行
            ，通常情况下，会选择第一个
            
2.Selector对象
    Selector 对象和SelectorList对象都有以下几种方法。

    extract() 返回选中内容的Unicode字符串

    extract_first()(SelectorList独有)

    返回列表中的第一个元素内容

    re("正则表达式") 正则提取

    re_first()(SelectorList独有)

    返回列表中的第一个元素内容


3.CSS
    response.css('css选择器').extract_first()   #获取一个

    response.css('css选择器').extract()  # 获取全部

    response.css('css选择器::attr(属性名)').extract()  # 获取其中某个属性

    (response.css('css选择器::text').extract()  #获取标签里的文本


4.xpath
    response.xpath('xpath选择器').extract_first()   #获取一个

    response.xpath('xpath选择器').extract()  # 获取全部

    .//a[contains(@class,"link-title")/text()]  # 获取文本

    .//a[contains(@class,"link-title")/@href]   #获取属性
```



**练习强化**

```python
###  crawlspider腾讯招聘爬虫
通过crawlspider爬取腾讯招聘的详情页的招聘信息
url：http://hr.tencent.com/position.php
```



## 代码示例：

```python
# -*- coding: utf-8 -*-
import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule
from qiubaiBycrawl.items import QiubaibycrawlItem
import re
class QiubaitestSpider(CrawlSpider):
    name = 'qiubaiTest'
    #起始url
    start_urls = ['http://www.qiushibaike.com/']

    #定义链接提取器，且指定其提取规则
    page_link = LinkExtractor(allow=r'/8hr/page/\d+/')
    
    rules = (
        #定义规则解析器，且指定解析规则通过callback回调函数
        Rule(page_link, callback='parse_item', follow=True),
    )

    #自定义规则解析器的解析规则函数
    def parse_item(self, response):
        div_list = response.xpath('//div[@id="content-left"]/div')
        
        for div in div_list:
            #定义item
            item = QiubaibycrawlItem()
            #根据xpath表达式提取糗百中段子的作者
            item['author'] = div.xpath('./div/a[2]/h2/text()').extract_first().strip('\n')
            #根据xpath表达式提取糗百中段子的内容
            item['content'] = div.xpath('.//div[@class="content"]/span/text()').extract_first().strip('\n')

            yield item #将item提交至管道
```

