## Item pipeline(管道文件)使用

当Item在Spider中被收集之后，它将会被传递到Item Pipeline，这些Item Pipeline组件按定义的顺序处理Item。

每个Item Pipeline都是实现了简单方法的Python类，比如决定此Item是丢弃而存储。以下是item pipeline的一些典型应用：

```shell
验证爬取的数据(检查item包含某些字段，比如说name字段)
查重(并丢弃)
将爬取结果保存到文件或者数据库中
```



item pipiline组件是一个独立的Python类，其中process_item()方法必须实现:

```python
class SomethingPipeline(object):
    def __init__(self):
        # 可选实现，做参数初始化等
        # doing something

    def process_item(self, item, spider):
        # item (Item 对象) – 被爬取的item
        # spider (Spider 对象) – 爬取该item的spider
        # 这个方法必须实现，每个item pipeline组件都需要调用该方法，
        # 这个方法必须返回一个 Item 对象，被丢弃的item将不会被之后的pipeline组件所处理。
        return item

    def open_spider(self, spider):
        # spider (Spider 对象) – 被开启的spider
        # 可选实现，当spider被开启时，这个方法被调用。

    def close_spider(self, spider):
        # spider (Spider 对象) – 被关闭的spider
        # 可选实现，当spider被关闭时，这个方法被调用
```

- 启用一个Item Pipeline组件 为了启用Item Pipeline组件，必须将它的类添加到 settings.py文件ITEM_PIPELINES 配置，就像下面这个例子:

```python
ITEM_PIPELINES = {
    #'mySpider.pipelines.SomePipeline': 300,
    "mySpider.pipelines.JobboleprojectPipeline":300
}
分配给每个类的整型值，确定了他们运行的顺序，item按数字从低到高的顺序，通过pipeline，通常将这些数字定义在0-1000范围内（0-1000随意设置，数值越低，组件的优先级越高）
```



## 管道启用存储数据库MongoDb

```python
from pymongo import MongoClient


class JobInfoPipeline(object):
    def open_spider(self, spider):
        """保存本地"""
        # if spider.name == 'job':
        #     self.file = open('job.txt', 'a+', encoding='utf-8')
        """保存到数据库"""
        if spider.name == 'job':
            self.mongo = MongoClient('192.168.109.131', 27017)
            self.db = self.mongo['job_data']['job']

    def process_item(self, item, spider):
        """保存本地"""
        # if spider.name == 'job':
        #     data = str(item)
        #     self.file.write(data)
        #     print('数据保存完成====logging！')
        # return item
        """保存到数据库"""
        # print(item, type(item))
        self.db.insert_one(dict(item))
        print('数据保存完成====logging！')

    def close_spider(self, spider):
        """保存本地"""
        # if spider.name == 'job':
        #     self.file.close()
        # #     self.mongo.close()
        """保存到数据库"""
        if spider.name == 'job':
            self.mongo.close()
```



## 管道启用存储数据库MySql

```python
往数据库里插数据
class ChinazspiderPipeline(object):
    def __init__(self):
        '''
        初始化方法
        '''
        # self.file = open('chinaz.json','a')
        # 创建数据库链接
        self.client = pymysql.Connect(
            '127.0.0.1', 'root', 'czj1234',
            'chinaz', 3306, charset='utf8'
        )
        # 创建游标
        self.cursor = self.client.cursor()

    def open_spider(self, spider):
        '''
        爬虫启动的时候回调用一次
        :param spider:
        :return:
        '''
        print('爬虫开启')
        pass

    def process_item(self, item, spider):
        '''
        这个方法必须实现,爬虫文件中所有的item 都会经过这个方法
        :param item: 爬虫文件传递过来的item对象
        :param spider: 爬虫文件实例化的对象
        :return:
        '''
        # 存储到本地json文件
        data_dict = dict(item)
        # import json
        # json_data = json.dumps(data_dict,ensure_ascii=False)
        # self.file.write(json_data+'\n')
        # 使用isisinstance判断item要存储的表
        # if isinstance(item,ChinazprojectWebInfoItem):
        #     print('网站信息')
        #     tablename = 'webinfo'
        # elif isinstance(item,ChinazspidertagItem):
        #     print('网站分类信息')
        #     tablename = 'tags'
        #     #
        # # 往数据库里写
        # sql = """
        #     insert into %s(%s)
        #     values (%s)
        # """ % (tablename,','.join(data_dict.keys()), ','.join(['%s'] * len(data_dict)))
        sql,data = item.get_insert_sql_data(data_dict)

        try:
            self.cursor.execute(sql, list(data_dict.values()))
            self.client.commit()
        except Exception as err:
            self.client.rollback()
            print(err)
        # 如果有多个管道文件,一定要return item , 否则下一管道无法接收到item
        print('经过了管道')
        return item

    def close_spider(self, spider):
        '''
        爬虫结束的时候调用一次
        :param spider:
        :return:
        '''
        # self.file.close()
        self.client.close()
        self.cursor.close()
        print('爬虫结束')
```



## 管道启用：异步插入数据库

twisted是一个异步的网络框架，这里可以帮助我们实现异步将数据插入数据库adbapi里面的子线程会去执行数据库的阻塞操作，
当一个线程执行完毕之后，同时，原始线程能继续进行正常的工作，服务其他请求。

```python
import pymysql
from twisted.enterprise import adbapi

#异步插入数据库
class DoubanPipeline(object):

    def __init__(self,dbpool):
        self.dbpool = dbpool

    #使用这个函数来应用settings配置文件。
    @classmethod
    def from_crawler(cls, crawler):
        parmas = {
        'host':crawler.settings['MYSQL_HOST'],
        'user':crawler.settings['MYSQL_USER'],
        'passwd':crawler.settings['MYSQL_PASSWD'],
        'db':crawler.settings['MYSQL_DB'],
        'port':3306,
        'charset':'utf8',
        }

        # **表示字典，*tuple元组,
        # 使用ConnectionPool，起始最后返回的是一个ThreadPool
        dbpool = adbapi.ConnectionPool(
            'pymysql',
            **parmas
        )
        return cls(dbpool)

    def process_item(self, item, spider):
        #这里去调用任务分配的方法
        query = self.dbpool.runInteraction(
            self.insert_data_todb,
            item,
            spider
        )
        #数据插入失败的回调
        query.addErrback(
            self.handle_error,
            item
        )

        #执行数据插入的函数
        def insert_data_todb(self,cursor,item,spider):
            insert_str,parmas = item.insertdata()
            cursor.execute(insert_str,parmas)
            print('插入成功')

    def handle_error(self,failure,item):
        print(failure)
        print('插入错误')
        #在这里执行你想要的操作

    def close_spider(self, spider):
        self.pool.close()
```


