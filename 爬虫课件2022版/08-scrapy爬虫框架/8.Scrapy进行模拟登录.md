## Scrapy进行模拟登录



## 使用登录后的cookie进行登录

```python
import scrapy
import re

class Login1Spider(scrapy.Spider):
    name = 'login1'
    allowed_domains = ['github.com']
    start_urls = ['https://github.com/NoobPythoner'] # 这是一个需要登陆以后才能访问的页面

    def start_requests(self): # 重构start_requests方法
        # 这个cookies_str是抓包获取的
        cookies_str = '...' # 抓包获取
        # 将cookies_str转换为cookies_dict
        cookies_dict = {i.split('=')[0]:i.split('=')[1] for i in cookies_str.split('; ')}
        yield scrapy.Request(
            self.start_urls[0],
            callback=self.parse,
            cookies=cookies_dict
        )

    def parse(self, response): # 通过正则表达式匹配用户名来验证是否登陆成功
        result_list = re.findall(r'noobpythoner|NoobPythoner', response.body.decode())
        print(result_list)
        pass
```

## Scrapy发送POST请求登录

```python
import scrapy
import re

class Login2Spider(scrapy.Spider):
   name = 'login2'
   allowed_domains = ['github.com']
   start_urls = ['https://github.com/login']

   def parse(self, response):
       authenticity_token = response.xpath("//input[@name='authenticity_token']/@value").extract_first()
       utf8 = response.xpath("//input[@name='utf8']/@value").extract_first()
       commit = response.xpath("//input[@name='commit']/@value").extract_first()

        #构造POST请求，传递给引擎
       yield scrapy.FormRequest(
           "https://github.com/session",
           formdata={
               "authenticity_token":authenticity_token,
               "utf8":utf8,
               "commit":commit,
               "login":"noobpythoner",
               "password":"***"
           },
           callback=self.parse_login
       )

   def parse_login(self,response):
       ret = re.findall(r"noobpythoner|NoobPythoner",response.text)
       print(ret)
```

## 提交表单：使用scrapy.Formrequest.from_response登陆github

```python
import scrapy
import re

class Login3Spider(scrapy.Spider):
    name = 'login3'
    allowed_domains = ['github.com']
    start_urls = ['https://github.com/login']

    def parse(self, response):
        yield scrapy.FormRequest.from_response(
            response, # 传入response对象,自动解析
            # 可以通过xpath来定位form表单,当前页只有一个form表单时,将会自动定位
            formxpath='//*[@id="login"]/form', 
            formdata={'login': 'noobpythoner', 'password': 'zhoudawei123'},
            callback=self.parse_login
        )

    def parse_login(self,response):
        ret = re.findall(r"noobpythoner|NoobPythoner", response.text)
        print(ret)
```

## Scrapy结合Selenium使用

原理：通过中间件的拦截，在中间件中进行selenium自动化的操作

```python
from scrapy import signals

# useful for handling different item types with a single interface
from itemadapter import is_item, ItemAdapter

from selenium import webdriver
from scrapy.http import HtmlResponse
import time


class WebdriverMiddleware(object):

    def process_request(self, request, spider):
        url = request.url
        # 通过浏览url地址，判断特征，获取过滤条件
        if 'daydata' in url:

            # 使用无头模式
            # option = ChromeOptions()
            # option.add_argument("--headless")
            #
            # webdriver防检测
            # option = ChromeOptions()
            # option.add_experimental_option('excludeSwitches', ['enable-automation'])
            # driver = webdriver.Chrome(options=option)
            # driver = webdriver.Chrome()
            # driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
            #     "source": """
            #     Object.defineProperty(navigator, 'webdriver', {
            #       get: () => undefined
            #     })
            #   """
            # })


            # # webdriver防检测
            # options = webdriver.ChromeOptions()
            #
            # # 无头模式
            # options.add_argument("--headless")
            #
            # options.add_experimental_option("excludeSwitches", ["enable-automation"])
            # options.add_experimental_option('useAutomationExtension', False)
            # driver = webdriver.Chrome(options=options)
            # driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
            #     "source": """
            #     Object.defineProperty(navigator, 'webdriver', {
            #       get: () => undefined
            #     })
            #   """
            # })



            """使用PhantomJS突破"""

            # driver = webdriver.PhantomJS()
            # 如果没有在环境变量指定PhantomJS浏览位置
            driver = webdriver.PhantomJS(r"C:\Users\宋\AppData\phantomjs-2.1.1-windows\bin\phantomjs.exe")

            driver.get(url)
            time.sleep(5)
            data = driver.page_source
            driver.close()
            # 创建响应对象

            return HtmlResponse(
                url=url,
                body=data,
                encoding='utf-8',
                request=request
            )
```

## @模式二启动：

```python
在spider目录下，进入爬虫文件
在__init__.py文件中，输入以下代码，即可通过run运行
from scrapy import cmdline
cmdline.execute('scrapy crawl job'.split(' '))
```

