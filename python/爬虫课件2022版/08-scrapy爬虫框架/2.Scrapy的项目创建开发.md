## Scrapy框架的安装

```shell
pip install scrapy
```

#### 安装报错：

```shell
# 报错内容：
        包含Twisted的异常:指的是这个库安装报错，无法通过pip进行安装
# 执行流程
	1.访问https://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted
	2.找到对应的解释器下载版本
		Twisted‑20.3.0‑cp37‑cp37m‑win_amd64.whl  这是python3.7的版本
		Twisted‑20.3.0‑cp38‑cp38‑win_amd64.whl   这是python3.8的版本
	3.下载之后，在该文件路径打开CMD窗口
	4.执行pip install Twisted‑20.3.0‑cp...  即可
	5.继续执行pip install scrapy
```



## Scrapy项目创建

```python
# 新建scrapy项目
# 爬虫项目创建
    # scrapy startproject 爬虫项目名
    # cd 爬虫项目名文件夹
    # scrapy genspider 爬虫名称 爬虫的域
# 爬虫名   开启/运行 爬虫的时候，需要通过爬虫名称开启爬虫
# 爬虫域   域名，告诉爬虫，我这个项目，只爬取属于什么域的内容
注意：爬虫项目名  与  爬虫名 不可同名
```



## scrapy文件树说明

<img src="../img/scrapy入门使用-1.png"></img>



## scrapy的爬虫文件说明

```python
import scrapy


class TestSpider(scrapy.Spider):
    # 爬虫名称
    name = 'test'
    # 设置允许爬取的域(可以指定多个)
    allowed_domains = ['baidu.com']
    # 设置起始url(设置多个)
    start_urls = ['http://baidu.com/']

    def parse(self, response):
        '''
        是一个回调方法,起始url请求成功后,会回调这个方法
        :param response: 响应结果
        :return:
        '''
        pass
"""parse是默认解析方法，默认解析start_request请求的响应，即解析地址是start_urls中的地址"""
```

**练习：创建百度首页的scrapy项目**



## yeild函数的说明

```python
# 生成器？
list1 = [i for i in range(1, 5)]
def index():
    for i in range(1, 5):
        yield i
f = index()
print(f.__next__())
print(f.__next__())
print(f.__next__())


# 什么是迭代器？
# list1 是一个可迭代对象
list1 = [1, 2, 3, 4, 5, 6]
# 创建可迭代对象
list_iter = iter(list1)
print(next(list_iter))
print(next(list_iter))
print(next(list_iter))
# 对已有的容器，迭代遍历，断点继续执行
```

**练习：scrapy爬取豆瓣TOP250电影信息**

```python
import scrapy
# 建模
from douban.items import DoubanItem


class DbSpider(scrapy.Spider):
    name = 'db'
    # 2.检查爬虫的域
    allowed_domains = ['douban.com']
    # 1.检查url队列的地址
    start_urls = ['https://movie.douban.com/top250']

    def parse(self, response):
        # 获取所有电影信息的节点
        # 打印request
        # print(response.request.headers['user-agent'])
        node_list = response.xpath('//ol[@class="grid_view"]/li')
        # print(len(node_list))
        item = DoubanItem()
        for data in node_list:
            # 电影的名称
            item['name'] = data.xpath('./div/div/div/a/span[1]/text()').extract_first()
            # 电影的详情页链接
            item['link'] = data.xpath('./div/div/div/a/@href').extract_first()
            # 电影的评分
            item['score'] = data.xpath('./div/div/div/div/span/text()').extract_first()
            # 电影的简介
            # .strip()    str方法，消除空格
            item['info'] = data.xpath('./div/div/div/p/text()[1]').extract_first().strip()
            print(item)
            yield item
        next_url = response.xpath('//span[@class="next"]/a/@href').extract_first()

        # 翻页
        if next_url is not None:
            # 构造request请求
            yield scrapy.Request(
                url=response.urljoin(next_url),
                callback=self.parse
            )
```



## 方法解析(重点)：

```xml-dtd
在xpath（）后使用extract（）可以返回所有的元素结果。
若xpath（）有问题，那么extract（）会返回一个空列表。
在xpath（）后使用extract_first（）可以返回第一个元素结果 

解析scrapy的response

parse()方法的参数 response 是start_urls里面的链接爬取后的结果。
所以在parse()方法中，我们可以直接对response对象包含的内容进行解析，比如浏览请求结果的网页源代码
或者进一步分析源代码内容，或者找出结果中的链接而得到下一个请求

response属性
    url ：HTTP响应的url地址,str类型
    status：HTTP响应的状态码, int类型
    headers ：HTTP响应的头部, 类字典类型, 可以调用get或者getlist方法对其进行访问
    body：HTTP响应正文, bytes类型
    text：文本形式的HTTP响应正文, str类型
        response.text = response.body.decode(response.encoding)
    encoding：HTTP响应正文的编码
    reqeust：产生该HTTP响应的Reqeust对象
    meta：即response.request.meta, 在构造Request对象时, 可将要传递给响应处理函数的信息通过meta参数传入, 响应处理函数处理响应时, 通过response.meta将信息提取出来
    urljoin(url) ：用于构造绝对url, 当传入的url参数是一个相对地址时, 根据response.url计算出相应的绝对url.

##### 参数解释
1. 中括号中的参数为可选参数
2. callback：表示当前的url的响应交给哪个函数去处理
3. meta：实现数据在不同的解析函数中传递，meta默认带有部分数据，比如下载延迟，请求深度等
4. dont_filter:默认为False，会过滤请求的url地址，即请求过的url地址不会继续被请求，对需要重复请求的url地址可以把它设置为Ture，比如贴吧的翻页请求，页面的数据总是在变化;start_urls中的地址会被反复请求，否则程序不会启动
5. method：指定POST或GET请求
6. headers：接收一个字典，其中不包括cookies
7. cookies：接收一个字典，专门放置cookies
8. body：接收一个字典，为POST的数据
```

