```python
分布式爬虫知识点总结
    scrapy_redis的含义和能够实现的功能
        1.scrapy是框架
        2.scrapy_redis是scrapy的组件
        3.scrapy_redis能够实现断点续爬和分布式爬虫
    scrapy_redis流程和实现原理
        1.在scrapy框架流程的基础上，把存储request对象放到了redis的有序集合中，利用该有序集合实现了请求队列
        2.并对request对象生成指纹对象，也存储到同一redis的集合中，利用request指纹避免发送重复的请求
    request对象进入队列的条件
        1.request的指纹不在集合中
        2.request的dont_filter为True，即不过滤
    request指纹的实现
        1.请求方法
        2.排序后的请求地址
        3.排序并处理过的请求体或空字符串
        4.用hashlib.sha1()对以上内容进行加密
    scarpy_redis实现增量式爬虫、布式爬虫
        1.对setting进行如下设置
            DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
            SCHEDULER = "scrapy_redis.scheduler.Scheduler"
            SCHEDULER_PERSIST = True
            ITEM_PIPELINES = {'scrapy_redis.pipelines.RedisPipeline': 400,}
            REDIS_URL = "redis://127.0.0.1:6379" # 请正确配置REDIS_URL
        2.爬虫文件中的爬虫类继承RedisSpider类
        3.爬虫类中redis_key替代了start_urls
        4.启动方式不同
            通过scrapy crawl spider启动爬虫后，向redis_key放入一个或多个起始url（lpush或rpush都可以），才能够让scrapy_redis爬虫运行
        5.除了以上差异点以外，scrapy_redis爬虫和scrapy爬虫的使用方法都是一样的


5.分布式爬虫实现
    不同点对比
        1.CrawlSpider 与 RedisCrawlSpider
        2.spider 与 RedisSpider

    分布式运行
        scrapy runspider + 爬虫.py文件
```