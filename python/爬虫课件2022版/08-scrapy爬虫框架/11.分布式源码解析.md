## @Scrapy_Redis源码解析

```python
下载官方的源码
    # 官方源码
    git clone https://github.com/rolando/scrapy-redis.git
    同时pip install scrapy-redis
```



```
scrapy_redis源码分析
    管道文件pipelines.py分析
    1.RedisPipeline(RedisPipeline中观察process_item，进行数据的保存，存入了redis中)
        使用了process_item的方法，实现数据的保存，与scrpay的管道pipelines.py一样的原理
        调用了一个异步线程去处理这个item
        做了保存到数据库的持久化处理(序列化处理)
```

<img src="../img/redis_pipeline.png"></img>

```
2.RFPDupeFilter(实现了对request对象的加密) ----> request_fingerprint_DupeFilter:请求对象指纹过滤器
    判断request对象是否存在，返回0表示已存在，反之，不存在
    使用了sha1加密，返回加密之后的16进制
```

<img src="../img/RFP.png"></img>

### @Redis数据库状态

<img src="../img/domz运行现象.png"></img>

```
3.Scheduler(scrapy_redis调度器的实现了决定什么时候把request对象加入带抓取的队列，同时把请求过的request对象过滤掉)
    如果在setting中设置为不持久，那么在爬虫执行完毕退出时，清空redis
    flush  ---> 集合清空，对request对象清空
    enqueue_request  ----> request对象过滤
    总结：request对象入队的条件
        1.request对象之前没有见过
        2.request的dont_filter为True，即不过滤
        3.start_urls中的url地址会入队，因为他们默认是不过滤
```

<img src="../img/scheduler.png"></img>


